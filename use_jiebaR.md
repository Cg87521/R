
最近又在做文本分析的一些东西，估摸着自己也已经很久没有好好写一篇文章了，于是打算好好总结一下目前所学的关于文本挖掘的知识。嗯，是基于R中的jiebaR这个package。

### 简介

R语言中文分词最好的选择目前应该就是jiebaR了，它的简介如下：

> 结巴分词(jiebaR)，是一款高效的R语言中文分词包，底层使用的是C++，通过Rcpp进行调用很高效。结巴分词基于MIT协议，就是免费和开源的，感谢国人作者的给力支持，让R的可以方便的处理中文文本。

分词是文本分析的第一步，所以jiebaR除了可以**分词**之外，还具备**词性标注**，**关键词提取**以及**文本Simhash相似度比较**等功能,其中在分词的时候还支持**自定义用户词典**以及**自定义停用词词典**。



### 安装及入门

我是通过在CRAN上直接安装:

```R
install.packages("jiebaR")
library("jiebaR")
```

先来感受一个分词的例子：

```R
分词器 = worker() 
segment("这是一段测试文本！", 分词器)
#> [1] "这是" "一段" "测试" "文本"
```

这样一句话就被分成了一个一个的词语。其中，worker()函数是用来初始化分词引擎的，它有很多参数，我们会在下面继续说明。

当然我们也可以添加一些自定义的词，比如“明天放假”这个四个字我们不想把它分为“明天”和“放假”，那么我们可以自己定义一个词叫做“明天放假”，这样jiebaR就会知道这是一个连贯的词。我们可以使用new_user_word()来添加我们想要定义的词。

```R
分词器 = worker()
segment("这是一个新词", 分词器)
#> [1] "这是" "一个" "新词"

# 第三个参数 "n" 代表新词的词性标记
new_user_word(分词器, "这是一个新词", "n") 
#> [1] TRUE
segment("这是一个新词", 分词器)
#> [1] "这是一个新词"
```

同时，我们也可以定义一些词，在分词的过程中删除这些词，因为它对我们之后的分析没有意义。我们使用stop_word参数来达到这一目的，比如：

```R
readLines("stop.txt")
#> [1] "停止"
分词器  = worker(stop_word = "stop.txt")
segment("这是一个停止词", 分词器)
#> [1] "这是" "一个" "词"
```

需要注意的是：

>!!!! 对于分词，请不要修改默认加载的停止词文本，即 jiebaR::STOPPATH，请使用自定义的停止词路径。

我是将stop.txt文件放在当前的工作目录下，请注意你在stop.txt文件中的换行符，推荐使用LR换行符，而不是CRLR换行符。

jiebaR提供了四种分词模式，分别是：

* 最大概率法mp（MPSegment），负责根据Trie树构建有向无环图和进行动态规划算法，是分词算法的核心。
* 隐式马尔科夫模型hmm（HMMSegment）是根据基于人民日报等语料库构建的HMM模型来进行分词，主要算法思路是根据(B,E,M,S)四个状态来代表每个字的隐藏状态。 HMM模型由dict/hmm_model.utf8提供。分词算法即viterbi算法。
* 混合模型mix（MixSegment）是四个分词引擎里面分词效果较好的类，结它合使用最大概率法和隐式马尔科夫模型。默认为混合模型
* 索引模型query（QuerySegment）先使用混合模型进行切词，再对于切出来的较长的词，枚举句子中所有可能成词的情况，找出词库里存在。

### 参数详解

worker() 用于初始化分词引擎，可以同时新建多个分词引擎。引擎的类型有： mix（混合模型）, mp（最大概率模型）, hmm（HMM模型）, query（索引模型）, tag（标记模型）, simhash（Simhash 模型）和 keywords（关键词模型），共7种。 各参数的默认值如下：

```R
worker(
type = "mix", 
dict = DICTPATH, 
hmm = HMMPATH, 
user = USERPATH,
idf = IDFPATH, 
stop_word = 
STOPPATH, 
write = T, 
qmax = 20, 
topn = 5,
encoding = "UTF-8", 
detect = T, 
symbol = F, 
lines = 1e+05,
output = NULL, 
bylines = F)
```
**type：**

mp（最大概率模型）- 基于词典和词频

hmm（HMM模型）- 基于 HMM 模型，可以发现词典中没有的词

mix（混合模型）- 先用 mp 分，mp 分完调用 hmm 再来把剩余的可能成词的单字分出来。

query（索引模型）- mix 基础上，对大于一定长度的词再进行一次切分。

tag（标记模型）- 词性标记，基于词典的

keywords（关键词模型）- tf-idf 抽 关键词

simhash（Simhash 模型） - 在关键词的基础上计算 simhash

**user：**

用户词典，包括词、词性标记两列。用户词典中的所有词的词频均为系统词典中的最大词频 (默认，可以通过 user_weight 参数修改)。

**stop_word**:

关键词提取使用的停止词库。分词时也可以使用，但是分词时使用的对应路径不能为默认的 jiebaR::STOPPATH。

**topn**

要提取的关键词数。

### 利用wordcloud2包画词云图

分词都做好了，怎么能不做个词云图看一下呢？我最开始画词云图是用朗大为开发的wordcloud包，说真的当时的词云图画出来挺丑的，不过好在后来朗老师又开发了wordcloud2包，画的词云图更加好看且灵活了。

wordcloud2的安装和使用都很简单，在这里就不再多说了，可以参考下面这篇文章：

* [可能是目前最好的词云解决方案 wordcloud2](https://cosx.org/2016/08/wordcloud2)

### 标记词性以及提取关键词

#### 标记词性

jiebaR还可以用来对词性进行标记，只需要在初始化引擎的时候设置type='tag'，如下：

```R
一段文本 = "我爱北京天安门"
标记器 = worker("tag")
结果 = tagging(一段文本, 标记器)
print(结果)
#>        r        v       ns       ns 
#>     "我"     "爱"   "北京" "天安门"
names(tagging(一段文本, 标记器))
#> [1] "r"  "v"  "ns" "ns"
```

#### 关键词提取

在进行关键词提取的时候，设置type='keywords'，topn 控制提取数量

```R
提取器 = worker("keywords", topn = 1)
keywords("我爱北京天安门", 提取器)
#>   8.9954 
#> "天安门"
```

####  Simhash 与海明距离

```R
摘要器 = worker("simhash", topn=2)
simhash("江州市长江大桥参加了长江大桥的通车仪式", 摘要器)
#> $simhash
#> [1] "12882166450308878002"
#> 
#> $keyword
#>    22.3853    8.69667 
#> "长江大桥"     "江州"
distance("hello world!", "江州市长江大桥参加了长江大桥的通车仪式", 摘要器)
#> $distance
#> [1] 23
#> 
#> $lhs
#> 11.7392 11.7392 
#> "hello" "world" 
#> 
#> $rhs
#>    22.3853    8.69667 
#> "长江大桥"     "江州"
```

对已经分好的词计算Simhash与海明距离：

```R
vector_simhash(c("今天","天气","真的","十分","不错","的","感觉"),摘要器)
#> $simhash
#> [1] "12098690169796312660"
#> 
#> $keyword
#> 6.45994 6.18823 
#>  "天气"  "不错"
vector_distance(c("今天","天气","真的","十分","不错","的","感觉"),c("今天","天气","真的","十分","不错","的","感觉"),摘要器)
#> $distance
#> [1] 0
#> 
#> $lhs
#> 6.45994 6.18823 
#>  "天气"  "不错" 
#> 
#> $rhs
#> 6.45994 6.18823 
#>  "天气"  "不错"
```
### TF-IDF算法

> TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。
>
> TFIDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。TFIDF实际上是：T*IDF，TF词频(Term Frequency)，IDF逆向文件频率(Inverse Document Frequency)。TF表示词条在文档d中出现的频率。IDF的主要思想是：如果包含词条t的文档越少，也就是n越小，IDF越大，则说明词条t具有很好的类别区分能力。

### Simhash 与海明距离

>传统IR领域内文本相似度比较所采用的经典方法是文本相似度的向量夹角余弦，其主要思想是根据一个文章中出现词的词频构成一个向量，然后计算两篇文章对应向量的向量夹角。但由于有可能一个文章的特征向量词特别多导致整个向量维度很高，使得计算的代价太大，对于Google这种处理万亿级别的网页的搜索引擎而言是不可接受的，simhash算法的主要思想是降维，将高维的特征向量映射成一个f-bit的指纹(fingerprint)，通过比较两篇文章的f-bit指纹的Hamming Distance来确定文章是否重复或者高度近似。



## 参考链接

* [simhash算法原理和代码实现](http://blog.sina.com.cn/s/blog_81e6c30b0101cpvu.html)
* [海量数据相似度计算之simhash和海明距离](http://www.lanceyan.com/tech/arch/simhash_hamming_distance_similarity.html)
* [TF-IDF与余弦相似性的应用（一）：自动提取关键词](http://www.ruanyifeng.com/blog/2013/03/tf-idf.html)
* [TF-IDF与余弦相似性的应用（二）：找出相似文章](http://www.ruanyifeng.com/blog/2013/03/cosine_similarity.html)
